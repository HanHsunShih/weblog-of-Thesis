# weblog-of-Thesis
This is the weblog of my thesis, where I document my iterative development and progress on a weekly basis

## 2, Oct - 6, Oct
### Struggled with running ControlNet on Google Colab Notebook
ControlNet requires high VRAM to run, so I upgraded to Colab Premium. I think maybe a dataset with 5k images and 5k prompt file is too big, I first wanted to make sure I can successfully run the notebook, so I decreased the dataset to 100 images with 100 corresponding textual prompts.

<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/2%2C%20Oct.png" alt="RTX 4070" width="800"/>

I kept searching for proper methods meet my intension, found fine-tuning is better for people who only have small dataset.
[Artical](https://new.qq.com/rain/a/20230403A020C800) I read introducing 4 ways to approach fine-tuning. I decided to try DreamBooth first.

## 9, Oct - 13, Oct

### Train DreamBooth of myself and Bobo my cat
According to the research I did last week, I found [this tutorial](https://www.youtube.com/watch?v=kCcXrmVk1F0&t=505s&ab_channel=MattWolfe) to train DreamBooth via Google Colab Notebook, I practiced to train portrait fo myself and Bobo, my cat, first to check the quality of the result. I used 20 images as train images, the result looks quite nice!
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/inject%20Amy.png" alt="RTX 4070" width="800"/>

### Collected dataset of my artworks
After the successful portrait model, I started to  collected images to build my drawing dataset from my previous artwork from 2015. I created a .psd file in Adobe Photoshop with file size as 512*512 pixels.

<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/FC%20dataset%20PS%20file.png" alt="RTX 4070" width="800"/>
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/73dataset.jpg" alt="RTX 4070" width="800"/>

### Train DreamBooth which present my drawing style
I first used a dataset which only included 10 images:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/DreamBooth%20dataset%2010%20images.png" alt="RTX 4070" width="800"/>

Here's the outcome:

<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/first%20DB%20outcome.png" alt="RTX 4070" width="800"/>

The outcome is very amazing! So I decided to dive into fine-tuning technoque.


### Installed Stable Diffusion Locally
If I decided to use fine-tuning technique, I would need to use Stable Diffusion since this technique is base on this. I can run Stable Diffusion on Google Colab Notebook and install it locally, I think install it locally might be more stable, so I followed [this tutorial](https://hossie.notion.site/Stable-Diffusion-MacBook-M1-M2-dda94dc6d59943ea8bc4108897642637) to install Stable Diffusion locally using terminal:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/install%20SD%20locally.png" alt="RTX 4070" width="800"/>


## 16, Oct - 20, Oct
### Train LoRA and generate suitable prompts
Last week I tried DreamBooth and found the result is quite nice! So this week I tried another way to do fine-tuning -- LoRA. I didn't choose Texture Inversion and Hypernetworks beacuse they are old techniques in terms of fine-tuning.
first I tryed many ways to generate suitable prompts for the image, prompts paired dataset in order to train LoRA model.
I followed [this tutorial](https://www.youtube.com/watch?v=fH1jf8juA8Y&t=242s&ab_channel=%E9%97%B9%E9%97%B9%E4%B8%8D%E9%97%B9), I couldn't installed the tagger extension mentioned in the video, I recorded what did I try in my diary:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/tagger%20extension.png" alt="RTX 4070" width="800"/>

Then I followed [this tutorial](https://www.youtube.com/watch?v=RgyOR5NiFMY&t=120s&ab_channel=%E6%8A%98%E9%A8%B0%E5%96%B5), tried 4 different ways using [Comparing image captioning models](https://huggingface.co/spaces/nielsr/comparing-captioning-models) online to generate prompts for this image:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/different%20prompt%20generator.png" alt="RTX 4070" width="800"/>
and also tried img2img Deep Booru in SD webui to generated prompt using same image as above, here's what Deep Booru generated:
**black hair, bubble, cat, dress, fish, multiple boys, ocean, planet, short hair, star \(sky\), starry sky, whale** ğŸ¤¦ğŸ»â€â™€ï¸ğŸ¤¦ğŸ»â€â™€ï¸ğŸ¤¦ğŸ»â€â™€ï¸

I would like the prompt to contain more details, so I eventually use GPT4's new function which can import image in communication box, here's the prompt generated by GPT4 with same image: **underwater, jellyfish, people, aquarium, sitting, watching, blue, marine life, glowing, bubbles, group of people, large window, ocean scene, colorful, tranquility, deep sea, sea creatures, mesmerized, stone arch, illumination, audience, relaxation, serene, aquatic** which is moch better.

So I use this method to complete my paired dataset.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/FishChief%20LoRA%20Dataset.png" alt="RTX 4070" width="800"/>

After having a proper dataset, I trained a series of LoRA models:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test1.png" alt="RTX 4070" width="800"/>
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test2.png" alt="RTX 4070" width="800"/>
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test3.png" alt="RTX 4070" width="800"/>
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test4.png" alt="RTX 4070" width="800"/>

The outcome is not similar to my drawing style, but at least I can now train my own LoRA model.


## 23, Oct - 27, Oct
### Switching to RunPod with Virtual GPU
I have a MacBook Pro with an M2 Pro chip. Although I successfully installed Stable Diffusion locally, I encountered persistent issues while attempting to train the LoRA due to the lack of an Nvidia GPU:
**error:raise AssertionError("Torch not compiled with CUDA enabled")AssertionError: Torch not compiled with CUDA enabled**

### Attempt 1
I installed Mambaforge, which is designed specifically for Apple Silicon (M1, M2 chips). To verify the installation, I ran  
```python -c "import torch; print(torch.cuda.is_available())" ```, and the output was **arm64**, indicating that PyTorch is running on Apple Silicon. However, I still faced the Torch not compiled with CUDA enabled error.

### Attempt 2
I tried editing the file at "/Users/hsun/Desktop/SDW/stable-diffusion-webui/venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py"
changing line 1273 from ```model.to(device)``` to ```model.to('cpu')```. Unfortunately, this didn't resolve the issue.

### Attempt 3
Luckily, there's a [cyber cafe](https://maps.app.goo.gl/wpHhDhVvc8A6ESWa8 "cyber cafe") near my home that provides computers with Windows systems and Nvidia RTX 4070 GPUs ğŸ¥³.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/IMG_1296%20(1).jpg?raw=true" alt="RTX 4070" width="400"/>

I followed a tutorial to install Stable Diffusion: (https://www.youtube.com/watch?v=onmqbI5XPH8 )
and everything ran very smoothly and quickly. However, I was not allowed to install **xformer**, a library designed for Transformer neural network architectures, due to an **Access Denied** error.

Consequently, I decided to use RunPod, a service where I can rent remote virtual machines with high-performance GPUs suitable for compute-intensive tasks such as machine learning and deep learning.


## 30, Oct - 3, Nov
### Used RunPod to train model
I watched this tutorial to learned how to install Stable Diffusion on RunPod: [Tutorial]( https://www.youtube.com/watch?v=a8WESfPwlYw&ab_channel=SECourses )
Also trained a DreamBooth model on virtual environment of Stable Diffusion on RunPod. I first picked RTX 3090 as GPU to deploy tamplete, then changed to RTX 4090 since it's significantly quicker and more powerful than the RTX 3090.

I followed [this tutorial](https://www.youtube.com/watch?v=g0wXIcRhkJk&t=890s&ab_channel=SECourses) to train DreamBooth on Runpod by using dataset I built in week 9, Oct - 13, Oct, following images are images generated by those DreamBooth checkpoints:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/DB%20checkpoint%20output.png" alt="RTX 4070" width="800"/>

### Used GPT4 to generated prompts
This week, I also trained some LoRA models via the [Google Colab Notebook](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) wtching this [tutorial](https://www.youtube.com/watch?v=oksoqMsVpaY&t=4s&ab_channel=Code%26bird).
LoRA model needs paired dataset which include images and corresponding textual prompts. I used GPT4 to help me generated the prompt then pasted the prompts into .txt file.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/spermwhale%20dataset.png" alt="RTX 4070" width="800"/>

### Train Sperm whale LoRA and Killer whale LoRA
I collected species 's images from CC0 website such as [pixabay](https://pixabay.com/), [Pexels](https://www.pexels.com/), pasted them into a .psd file in Adobe Photoshop:
<img src="" alt="RTX 4070" width="400"/>

then use ChatGPT to help me write the code in Google Colab Notebook to export images from .psd file:
```
!pip install psd-tools
!pip install psd-tools Pillow

from google.colab import drive
drive.mount('/content/drive')

from PIL import Image
import os
from psd_tools import PSDImage

# è®€å–PSDæª”æ¡ˆ
psd_path = '/content/drive/MyDrive/Thesis/fishchief_style.psd'
psd = PSDImage.open(psd_path)

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
output_dir = '/content/drive/MyDrive/Thesis/1102LoRA dataset'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

canvas_width, canvas_height = 512, 512

# å°å‡ºæ¯ä¸€å€‹åœ–å±¤
for i, layer in enumerate(psd, start=1):
    # å–å¾—åœ–å±¤çš„åœ–åƒ
    img = layer.topil()

    # è‹¥åœ–å±¤ç‚ºé€æ˜æˆ–æ²’æœ‰åœ–ç‰‡ï¼Œè·³éæ­¤åœ–å±¤
    if img is None:
        continue

    # å¦‚æœåœ–å±¤å¸¶æœ‰é€æ˜åº¦ï¼Œå‰‡å°‡é€æ˜éƒ¨åˆ†å¡«å……ç‚ºç™½è‰²
    if img.mode == 'RGBA':
        # å‰µå»ºä¸€å€‹ç™½è‰²èƒŒæ™¯
        white_background = Image.new("RGBA", img.size, "WHITE")
        # çµ„åˆåœ–å±¤èˆ‡ç™½è‰²èƒŒæ™¯
        img = Image.alpha_composite(white_background, img)

    # è½‰æ›åˆæˆå¾Œçš„åœ–ç‰‡ç‚ºRGB
    img_rgb = img.convert("RGB")

    # å‰µå»ºä¸€å€‹512x512çš„ç™½è‰²èƒŒæ™¯åœ–åƒ
    white_bg = Image.new("RGB", (canvas_width, canvas_height), "WHITE")

    # è€ƒæ…®åœ–å±¤çš„ä½ç½®ï¼Œç¢ºä¿åœ–å±¤ä¿æŒåœ¨åŸä¾†çš„ä½ç½®
    paste_position = (layer.left, layer.top)

    # ç²˜è²¼åœ–å±¤åˆ°èƒŒæ™¯ä¸Š
    white_bg.paste(img_rgb, paste_position)

    # è¨­å®šè¼¸å‡ºè·¯å¾‘
    output_path = os.path.join(output_dir, f"{i}.jpg")

    # å°å‡ºåœ–åƒ
    white_bg.save(output_path, 'JPEG')

    # å‰µå»ºå°æ‡‰çš„ç©ºç™½TXTæª”æ¡ˆ
    txt_output_path = os.path.join(output_dir, f"{i}.txt")
    with open(txt_output_path, 'w') as fp:
        pass  # 'pass'æ„å‘³è‘—ä¸åšä»»ä½•äº‹æƒ…ï¼Œç•™ä¸‹ä¸€å€‹ç©ºæ–‡ä»¶

print(f"Layers exported to {output_dir}")
```
Then I followed [this tutorial](https://www.youtube.com/watch?v=oksoqMsVpaY&t=4s&ab_channel=Code%26bird) to train LoRA for secific species.
I trained loads of LoRA and saved them in drive:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20models%20in%20drive.png" alt="RTX 4070" width="800"/>

### Combine DreamBooth checkpoint with LoRA and do some experimentations
I used FishChief checkpoint as base model and combined with species LoRA, FishChief stylish LoRA to generate images. By using different parameters, optimizers, sampling steps and prompts, I generated loads of images similar to FishChief's illustration style.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/experimentations.jpg" alt="RTX 4070" width="800"/>

<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/Final%20outcome%20of%20the%20thesis.png" alt="RTX 4070" width="800"/>

### Learned how to use img2img tag function
img2img function in stable diffusion allow users to modify, enhance, or transform an existing image based on a given textual prompt. I used __inpainted__ function in this tag to re-generate whale's tail if its not correct.
[tutorial](https://www.youtube.com/watch?v=hMvZsAaF7Gs&ab_channel=%E5%B0%8F%E9%BB%91Leo)

### Other new things I learned this week
__Hi-res__: Many images in CIVITAI have an icon saying "Hi-res", it refers to a technique used to improve the quality of the generated images, especially at higher resolutions.
(https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/6509)

__Manage LoRA models__: LoRA can only be activated when the trigger words are correct being used in the text, [lora-prompt-tool](https://github.com/a2569875/lora-prompt-tool) is a useful tool have following features: Automatic add trigger words to prompts/ Prompt search/filtering/ Editing and managing prompts/ Batch import of prompts which could be useful when having many LoRA models.








