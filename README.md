# weblog-of-Thesis
This is the weblog of my thesis, where I document my iterative development and progress on a weekly basis


## 9, Oct - 13, Oct
### Collected dataset
This week I collected images to build my dataset from my previous artwork. I created a .psd file in Adobe Photoshop with file size as 512*512 pixels.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/FC%20dataset%20PS%20file.png" alt="RTX 4070" width="800"/>



### Installed Stable Diffusion Locally


## 16, Oct - 20, Oct
### Generate suitable prompts
This week I tryed many ways to generate suitable prompts for the image, prompts paired dataset in order to train LoRA model.
I first followed [this tutorial](https://www.youtube.com/watch?v=fH1jf8juA8Y&t=242s&ab_channel=%E9%97%B9%E9%97%B9%E4%B8%8D%E9%97%B9), I couldn't installed the tagger extension mentioned in the video, I recorded what did I try in my diary:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/tagger%20extension.png" alt="RTX 4070" width="800"/>

Then I followed [this tutorial](https://www.youtube.com/watch?v=RgyOR5NiFMY&t=120s&ab_channel=%E6%8A%98%E9%A8%B0%E5%96%B5), tried 4 different ways using [Comparing image captioning models](https://huggingface.co/spaces/nielsr/comparing-captioning-models) online to generate prompts for this image:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/different%20prompt%20generator.png" alt="RTX 4070" width="800"/>
and also tried img2img Deep Booru in SD webui to generated prompt using same image as above, here's what Deep Booru generated:
**black hair, bubble, cat, dress, fish, multiple boys, ocean, planet, short hair, star \(sky\), starry sky, whale** ü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏è

I would like the prompt to contain more details, so I eventually use GPT4's new function which can import image in communication box, here's the prompt generated by GPT4 with same image: **underwater, jellyfish, people, aquarium, sitting, watching, blue, marine life, glowing, bubbles, group of people, large window, ocean scene, colorful, tranquility, deep sea, sea creatures, mesmerized, stone arch, illumination, audience, relaxation, serene, aquatic** which is moch better.

So I use this method to complete my paired dataset.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/FishChief%20LoRA%20Dataset.png" alt="RTX 4070" width="800"/>

After having a proper dataset, I trained a series of LoRA models:
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test1.png" alt="RTX 4070" width="800"/>
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test2.png" alt="RTX 4070" width="800"/>
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test3.png" alt="RTX 4070" width="800"/>
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/LoRA%20model%20test4.png" alt="RTX 4070" width="800"/>
The outcome is not similar to my drawing style, but at least I can now train my own LoRA model.


## 23, Oct - 27, Oct
### Switching to RunPod with Virtual GPU
I have a MacBook Pro with an M2 Pro chip. Although I successfully installed Stable Diffusion locally, I encountered persistent issues while attempting to train the LoRA due to the lack of an Nvidia GPU:
**error:raise AssertionError("Torch not compiled with CUDA enabled")AssertionError: Torch not compiled with CUDA enabled**

### Attempt 1
I installed Mambaforge, which is designed specifically for Apple Silicon (M1, M2 chips). To verify the installation, I ran  
```python -c "import torch; print(torch.cuda.is_available())" ```, and the output was **arm64**, indicating that PyTorch is running on Apple Silicon. However, I still faced the Torch not compiled with CUDA enabled error.

### Attempt 2
I tried editing the file at "/Users/hsun/Desktop/SDW/stable-diffusion-webui/venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py"
changing line 1273 from ```model.to(device)``` to ```model.to('cpu')```. Unfortunately, this didn't resolve the issue.

### Attempt 3
Luckily, there's a [cyber cafe](https://maps.app.goo.gl/wpHhDhVvc8A6ESWa8 "cyber cafe") near my home that provides computers with Windows systems and Nvidia RTX 4070 GPUs ü•≥.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/IMG_1296%20(1).jpg?raw=true" alt="RTX 4070" width="400"/>

I followed a tutorial to install Stable Diffusion: (https://www.youtube.com/watch?v=onmqbI5XPH8 )
and everything ran very smoothly and quickly. However, I was not allowed to install **xformer**, a library designed for Transformer neural network architectures, due to an **Access Denied** error.

Consequently, I decided to use RunPod, a service where I can rent remote virtual machines with high-performance GPUs suitable for compute-intensive tasks such as machine learning and deep learning.


## 30, Oct - 3, Nov
### Used RunPod to train model
I watched this tutorial to learned how to install Stable Diffusion on RunPod: [Tutorial]( https://www.youtube.com/watch?v=a8WESfPwlYw&ab_channel=SECourses )
Also trained a DreamBooth model. I first picked RTX 3090 as GPU to deploy tamplete, then changed to RTX 4090 since it's significantly quicker and more powerful than the RTX 3090.

### Used GPT4 to generated prompts
This week, I also trained some LoRA models via the [Google Colab Notebook](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) wtching this [tutorial](https://www.youtube.com/watch?v=oksoqMsVpaY&t=4s&ab_channel=Code%26bird).
LoRA model needs paired dataset which include images and corresponding textual prompts. I used GPT4 to help me generated the prompt then pasted the prompts into .txt file.
<img src="https://github.com/HanHsunShih/weblog-of-Thesis/blob/main/images/spermwhale%20dataset.png" alt="RTX 4070" width="800"/>

### Train Sperm whale LoRA and Killer whale LoRA
I collected species 's images from CC0 website such as [pixabay](https://pixabay.com/), [Pexels](https://www.pexels.com/), pasted them into a .psd file in Adobe Photoshop:
<img src="" alt="RTX 4070" width="400"/>

then use ChatGPT to help me write the code in Google Colab Notebook to help 

### Learned how to use img2img tag function
img2img function in stable diffusion allow users to modify, enhance, or transform an existing image based on a given textual prompt. I used __inpainted__ function in this tag to re-generate whale's tail if its not correct.
[tutorial](https://www.youtube.com/watch?v=hMvZsAaF7Gs&ab_channel=%E5%B0%8F%E9%BB%91Leo)

### Other new things I learned this week
__Hi-res__: Many images in CIVITAI have an icon saying "Hi-res", it refers to a technique used to improve the quality of the generated images, especially at higher resolutions.
(https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/6509)

__Manage LoRA models__: LoRA can only be activated when the trigger words are correct being used in the text, [lora-prompt-tool](https://github.com/a2569875/lora-prompt-tool) is a useful tool have following features: Automatic add trigger words to prompts/ Prompt search/filtering/ Editing and managing prompts/ Batch import of prompts which could be useful when having many LoRA models.








